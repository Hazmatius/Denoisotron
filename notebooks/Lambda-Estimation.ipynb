{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import torch\n",
    "import kornia\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from helpers import Trainer\n",
    "from helpers import Logger\n",
    "from helpers import Trial\n",
    "from mibi_dataloader import MIBIData\n",
    "from modules import SelfSupervisedEstimator\n",
    "from criteria import SelfSupervisedEstimatorLoss\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "main_dir = '/home/hazmat/GitHub/Denoisotron/'\n",
    "train_dir = main_dir + 'data/traindat/'\n",
    "test_dir = main_dir + 'data/testdat/'\n",
    "\n",
    "modl_dir = main_dir + 'models/'\n",
    "rslt_dir = main_dir + 'results/'\n",
    "\n",
    "# train_ds = MIBIData(folder=train_dir, crop=31, scale=1, stride=8, number=10000)\n",
    "# test_ds = MIBIData(folder=test_dir, crop=31, scale=1, stride=8)\n",
    "lambda_ds = MIBIData.depickle('/home/hazmat/GitHub/Denoisotron/data/large_lambda_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def l_est(samples, bs, ts):\n",
    "#     l_ests = torch.zeros(samples.shape)\n",
    "#     for i in range(samples.size(0)):\n",
    "#         l_ests[i,:,:,:], cimg = utils.estimate_lambda(samples[i,0,:,:], bs, ts)\n",
    "#         print('\\rProcessing.......' + str(100 * i / samples.size(0)) + '%', end='')\n",
    "#     return l_ests\n",
    "\n",
    "# bs = [10];\n",
    "# while bs[-1]>.7:\n",
    "#     bs.append(bs[-1]*0.8)\n",
    "\n",
    "# ts = torch.tensor(bs).float()**-2\n",
    "# bs = np.array(bs)\n",
    "# ts = ts.numpy()\n",
    "\n",
    "# l_images = l_est(train_ds.images, bs, ts)\n",
    "\n",
    "# lambda_ds = MIBIData(images=l_images, labels='None', source=train_ds.source, crop=31, scale=1, stride=8)\n",
    "# lambda_ds.pickle('/home/hazmat/GitHub/Denoisotron/data/large_lambda_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator_args = dict()\n",
    "# estimator_args['kdims'] =  [15]\n",
    "# estimator_args['nfilts'] = [0]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# estimator = LambdaEstimator(**estimator_args)\n",
    "# estimator = LambdaEstimator.load_model(main_dir + 'estimator/models/estimator_saves/', 'model_109')\n",
    "# estimator = SelfSupervisedEstimator(**estimator_args)\n",
    "estimator = SelfSupervisedEstimator.load_model(main_dir + 'estimator/models/estimator_saves/', 'amazing_model')\n",
    "\n",
    "estimator.cuda()\n",
    "estimator_logger = Logger(['loss'])\n",
    "estimator_trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n"
     ]
    }
   ],
   "source": [
    "# Estimator training parameters\n",
    "estimator_train_args = dict()\n",
    "estimator_train_args['lr'] = 0.0001\n",
    "estimator_train_args['batch_size'] = 100\n",
    "estimator_train_args['epochs'] = 10\n",
    "estimator_train_args['report'] = 5\n",
    "estimator_train_args['crop'] = 121\n",
    "estimator_train_args['clip'] = 1\n",
    "estimator_train_args['decay'] = 0\n",
    "estimator_train_args['restart'] = False\n",
    "estimator_train_args['epoch_frac'] = 1\n",
    "# estimator_train_args['error_target'] = 0.4\n",
    "# estimator_train_args['decay'] = 1e-5\n",
    "\n",
    "# Estimator loss parameters\n",
    "estimator_criterion = SelfSupervisedEstimatorLoss()\n",
    "\n",
    "lambda_ds.set_crop(estimator_train_args['crop'])\n",
    "\n",
    "# estimator.noise = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n",
      "Epoch:0 > < 0.02773431102956276                                                                                                                    \n",
      "Epoch:1 > < 0.02767236893478373                                                                                                                    \n",
      "Epoch:2 > < 0.02764169652295472                                                                                                                    \n",
      "Epoch:3 > < 0.027620545459274063                                                                                                                   \n",
      "Epoch:4 > < 0.027605187734290312                                                                                                                   \n",
      "Epoch:5 > < 0.027595929520206893                                                                                                                   \n",
      "Epoch:6 > < 0.027582977054445214                                                                                                                   \n",
      "Epoch:7 > < 0.027577716641814225                                                                                                                   \n",
      "Epoch:8 > < 0.027571929519459817                                                                                                                   \n",
      "Epoch:9 > < 0.027565095774717344                                                                                                                   \n",
      "trained in 278825.5082011223 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "278825.5082011223"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.cuda()\n",
    "estimator_train_args['continue'] = True\n",
    "estimator_trainer.train(estimator, lambda_ds, estimator_criterion, estimator_logger, main_dir + 'estimator/models/', **estimator_train_args)\n",
    "# print(estimator.noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filt_fig = plt.figure()\n",
    "# filt = estimator.filters[10].lambda_estimator.conv.weight.cpu().detach()[0,0,:,:]\n",
    "# print(torch.sum(filt))\n",
    "# plt.imshow(filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator.cuda()\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "data = test_ds.images[271].unsqueeze(0)\n",
    "lamd = estimator.process(data.cuda())\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "plt.imshow(data[0,0,:,:].cpu().detach())\n",
    "ax2 = plt.subplot(1,2,2, sharex=ax1, sharey=ax1)\n",
    "plt.imshow(lamd[0,0,:,:].cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AABAAAAPoint1_3_Abeta40.tif\n",
    "# print(train_ds.source[1])\n",
    "data = train_ds.images[2].unsqueeze(0).cuda()\n",
    "l_hat = estimator.process(data)\n",
    "l_hat[l_hat<0]=0\n",
    "resample = torch.tensor(np.random.poisson(l_hat.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# estimator.eval()\n",
    "# estimator.cpu()\n",
    "# msecrit = torch.nn.MSELoss()\n",
    "# data = get_even_data(test_ds, label_dict).cpu()\n",
    "# n_iters = 20\n",
    "# r_data_1 = torch.tensor(data).cpu();\n",
    "# r_data_2 = torch.tensor(data).cpu();\n",
    "# error_1 = list()\n",
    "# error_2 = list()\n",
    "# data_hist_1 = list()\n",
    "# data_hist_2 = list()\n",
    "# est_hist_1 = list()\n",
    "# est_hist_2 = list()\n",
    "\n",
    "# for i in range(n_iters):\n",
    "#     r_data_1 = gdt_estimator(r_data_1.cpu(), bs, ts).cpu()\n",
    "#     r_data_2 = net_estimator(r_data_2, estimator).cpu()\n",
    "#     est_hist_1.append(r_data_1)\n",
    "#     est_hist_2.append(r_data_2)\n",
    "#     # mean_val = torch.mean(r_data)\n",
    "#     loss_1 = msecrit(r_data_1, data)\n",
    "#     loss_2 = msecrit(r_data_2, data)\n",
    "    \n",
    "#     error_1.append(loss_1.item())\n",
    "#     error_2.append(loss_2.item())\n",
    "    \n",
    "#     r_data_1 = torch.poisson(r_data_1.detach()).cpu()\n",
    "#     r_data_2 = torch.poisson(r_data_2.detach()).cpu()\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to specify a model\n",
    "# we should sample multiple times at each model, on each iteration, for each sample\n",
    "\n",
    "def get_data_even(dataset, label_dict):\n",
    "    avoid_label = ['Au', 'Background', 'C', 'Ca', 'Fe', 'Na', 'Si', 'Ta', 'totalIon']\n",
    "    idxs = list()\n",
    "    for label in label_dict.keys():\n",
    "        if not any(label==avoid for avoid in avoid_label):\n",
    "            idx = np.random.choice(label_dict[label])\n",
    "            idxs.append(idx)\n",
    "    data = dataset.images[idxs]\n",
    "    return data.cuda()\n",
    "            \n",
    "# randomly pick some data\n",
    "def get_data(dataset, n):\n",
    "    idxs = np.random.permutation(len(dataset.images))\n",
    "    idxs = idxs[0:n]\n",
    "    data = torch.tensor(dataset.images[idxs])\n",
    "    return data.cuda()\n",
    "\n",
    "def gdt_estimator(samples, **kwargs):\n",
    "    bs = kwargs['bs']\n",
    "    ts = kwargs['ts']\n",
    "    l_ests = torch.zeros(samples.shape)\n",
    "    for i in range(samples.size(0)):\n",
    "        l_ests[i,:,:,:], cimg = utils.estimate_lambda(samples[i,0,:,:], bs, ts)\n",
    "    return l_ests\n",
    "\n",
    "def net_estimator(samples, **kwargs):\n",
    "    net = kwargs['network']\n",
    "    l_ests = torch.zeros(samples.shape)\n",
    "    for i in range(samples.size(0)):\n",
    "        with torch.no_grad():\n",
    "            l_ests[i,:,:,:] = net.process(samples[i,:,:,:].unsqueeze(0).cuda())\n",
    "    return l_ests\n",
    "\n",
    "def stupid_estimator(samples, **kwargs):\n",
    "    l_ests = torch.zeros(samples.shape)\n",
    "    for i in range(samples.size(0)):\n",
    "        l_ests[i,:,:,:] = torch.tensor(samples[i,:,:,:])\n",
    "    return l_ests\n",
    "\n",
    "def naive_estimator(samples, **kwargs):\n",
    "    blur = kwargs['blur']\n",
    "    l_ests = torch.zeros(samples.shape)\n",
    "    for i in range(samples.size(0)):\n",
    "        l_ests[i,:,:,:] = utils.imgaussfilt(samples[i,0,:,:], blur).unsqueeze(0).unsqueeze(0)\n",
    "    return l_ests\n",
    "\n",
    "def iterated_test(data, n_iters, estimator_function, **kwargs):\n",
    "    msecrit = torch.nn.MSELoss()\n",
    "    rdata = data.clone().detach()\n",
    "    error_hist = list()\n",
    "    l_est_hist = list()\n",
    "    rdata_hist = list()\n",
    "    for i in range(n_iters):\n",
    "        l_est = estimator_function(rdata, **kwargs)\n",
    "        l_est_hist.append(l_est.cpu().detach())\n",
    "        loss = msecrit(l_est.cuda(), data.cuda())\n",
    "        error_hist.append(loss.item())\n",
    "        rdata = torch.poisson(l_est.detach()).cpu()\n",
    "        rdata_hist.append(rdata)\n",
    "        print('   ', i)\n",
    "    results = {}\n",
    "    results['data'] = data\n",
    "    results['error_hist'] = error_hist\n",
    "    results['l_est_hist'] = l_est_hist\n",
    "    results['rdata_hist'] = rdata_hist\n",
    "    return results\n",
    "\n",
    "def iterated_test_synthetic(data, n_iters, baseline_estimator, estimator_function, **kwargs):\n",
    "    msecrit = torch.nn.MSELoss()\n",
    "    baseline = net_estimator(data, network=baseline_estimator) # synthetic data\n",
    "    rdata = torch.poisson(baseline) # simulating a MIBI run\n",
    "    error_hist = list()\n",
    "    l_est_hist = list()\n",
    "    rdata_hist = list()\n",
    "    for i in range(n_iters):\n",
    "        rdata_hist.append(rdata) # save the 'MIBI' data\n",
    "        l_est = estimator_function(rdata, **kwargs) # estimate lambda\n",
    "        l_est_hist.append(l_est.cpu().detach()) # save the estimate\n",
    "        loss = msecrit(l_est.cuda(), data.cuda()) # get the loss\n",
    "        error_hist.append(loss.item()) # save the loss\n",
    "        rdata = torch.poisson(l_est.detach()).cpu() # simulate a MIBI run\n",
    "        print('   ', i)\n",
    "    results = {}\n",
    "    results['data'] = data\n",
    "    results['error_hist'] = error_hist\n",
    "    results['l_est_hist'] = l_est_hist\n",
    "    results['rdata_hist'] = rdata_hist\n",
    "    return results\n",
    "    \n",
    "import re\n",
    "\n",
    "def get_label(string):\n",
    "    parts = re.split('_|\\.', string)\n",
    "    return parts[1]\n",
    "\n",
    "label_dict = {}\n",
    "for i in range(len(test_ds.source)):\n",
    "    label = get_label(test_ds.source[i])\n",
    "    if label in label_dict:\n",
    "        label_dict[label].append(i)\n",
    "    else:\n",
    "        label_dict[label] = [i]\n",
    "\n",
    "    \n",
    "    \n",
    "bs = [10];\n",
    "while bs[-1]>.7:\n",
    "    bs.append(bs[-1]*0.8)\n",
    "ts = torch.tensor(bs).float()**-2\n",
    "bs = np.array(bs)\n",
    "ts = ts.numpy()\n",
    "\n",
    "estimator.eval()\n",
    "estimator.cuda()\n",
    "data = get_data_even(test_ds, label_dict).cpu()\n",
    "n_iters = 20\n",
    "\n",
    "print('running net_estimator...')\n",
    "net_estimator_results = iterated_test_synthetic(data, n_iters, estimator, net_estimator, network=estimator)\n",
    "print()\n",
    "print('running gdt_estimator...')\n",
    "gdt_estimator_results = iterated_test_synthetic(data, n_iters, estimator, gdt_estimator, bs=bs, ts=ts)\n",
    "print()\n",
    "print('running stupid_estimator...')\n",
    "stupid_estimator_results = iterated_test_synthetic(data, n_iters, estimator, stupid_estimator)\n",
    "print()\n",
    "print('running naive_estimator with blur=1...')\n",
    "naive_estimator_1_results = iterated_test_synthetic(data, n_iters, estimator, naive_estimator, blur=1)\n",
    "print()\n",
    "print('running naive_estimator with blur=5...')\n",
    "naive_estimator_5_results = iterated_test_synthetic(data, n_iters, estimator, naive_estimator, blur=5)\n",
    "print()\n",
    "print('running naive_estimator with blur=20...')\n",
    "naive_estimator_20_results = iterated_test_synthetic(data, n_iters, estimator, naive_estimator, blur=20)\n",
    "print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# def jar_data(folder, file, data):\n",
    "#     with open(folder + file, 'wb') as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "# save_folder = '/home/hazmat/GitHub/Denoisotron/data/'\n",
    "# jar_data(save_folder, 'synthetic_net_estimator_results', net_estimator_results)\n",
    "# jar_data(save_folder, 'synthetic_gdt_estimator_results', gdt_estimator_results)\n",
    "# jar_data(save_folder, 'synthetic_stupid_estimator_results', stupid_estimator_results)\n",
    "# jar_data(save_folder, 'synthetic_naive_estimator_1_results', naive_estimator_1_results)\n",
    "# jar_data(save_folder, 'synthetic_naive_estimator_5_results', naive_estimator_5_results)\n",
    "# jar_data(save_folder, 'synthetic_naive_estimator_20_results', naive_estimator_20_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist2d(x_data, y_data, edges):\n",
    "    h, xedges, yedges = np.histogram2d(x_data, y_data, bins=edges)\n",
    "    h = h.T\n",
    "    sum_val = torch.tensor(np.sum(h,0)).unsqueeze(0).repeat(len(edges)-1,1)\n",
    "    h = torch.tensor(h)/sum_val\n",
    "    h = np.nan_to_num(h.numpy(), 0)\n",
    "    return h, xedges, yedges\n",
    "\n",
    "\n",
    "def plot_error_hist(network, data, **kwargs):\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    l = net_estimator(data, network=network)\n",
    "    smibi_data = torch.poisson(l).cpu()\n",
    "    l_hat = net_estimator(smibi_data, network=network)\n",
    "    \n",
    "    error = torch.abs(l_hat-l)\n",
    "    data_min = torch.min(l)\n",
    "    data_max = torch.max(l)\n",
    "    error_min = torch.min(error)\n",
    "    error_max = torch.max(error)\n",
    "    \n",
    "    xedges = np.linspace(data_min, data_max, 100)\n",
    "    yedges = np.linspace(error_min, error_max, 100)\n",
    "    x_data = l.flatten().numpy()\n",
    "    y_data = error.flatten().numpy()\n",
    "    \n",
    "    h, x_edges, y_edges = get_hist2d(x_data, y_data, [xedges, yedges])\n",
    "    \n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    plt.imshow(l[0,0,:,:])\n",
    "    plt.axis('off')\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    img = plt.imshow(h, origin='low', extent=[x_edges[0], x_edges[-1], y_edges[0], y_edges[-1]], **kwargs)\n",
    "    plt.plot([0,x_edges[-1]], [0,x_edges[-1]], color='red')\n",
    "    plt.plot([0,x_edges[-1]], [0,-x_edges[-1]], color='red')\n",
    "    ax2.set_aspect(x_edges[-1]/y_edges[-1])\n",
    "    plt.xlim(x_edges[0], x_edges[-1])\n",
    "    plt.ylim(y_edges[0], y_edges[-1])\n",
    "    fig.colorbar(img, ax=ax2)\n",
    "\n",
    "\n",
    "def plot_2_error_hist(baseline_estimator, baseline_estimator_args, data, estimator_1, args_1, estimator_2, args_2, **kwargs):\n",
    "    l = baseline_estimator(data, **baseline_estimator_args)\n",
    "    x_data = l.flatten().numpy()\n",
    "    smibi_data = torch.poisson(l).cpu()\n",
    "    \n",
    "    l_hat_1 = estimator_1(smibi_data, **args_1)\n",
    "    error_1 = torch.abs(l_hat_1 - l)\n",
    "    y_data_1 = error_1.flatten().numpy()\n",
    "    error_1_min = torch.min(error_1)\n",
    "    error_1_max = torch.max(error_1)\n",
    "    \n",
    "    l_hat_2 = estimator_2(smibi_data, **args_2)\n",
    "    error_2 = torch.abs(l_hat_2 - l)\n",
    "    y_data_2 = error_2.flatten().numpy()\n",
    "    error_2_min = torch.min(error_2)\n",
    "    error_2_max = torch.max(error_2)\n",
    "    \n",
    "    data_min = torch.min(l)\n",
    "    data_max = torch.max(l)\n",
    "    error_min = torch.min(error_1_min, error_2_min)\n",
    "    error_max = torch.max(error_1_max, error_2_max)\n",
    "    \n",
    "    xedges = np.linspace(data_min, data_max, 100)\n",
    "    yedges = np.linspace(error_min, error_max, 100)\n",
    "    \n",
    "    h1, _, _ = get_hist2d(x_data, y_data_1, [xedges, yedges])\n",
    "    h2, _, _ = get_hist2d(x_data, y_data_2, [xedges, yedges])\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    \n",
    "    ax1 = plt.subplot(2,3,1)\n",
    "    img1 = plt.imshow(l[0,0,:,:])\n",
    "    plt.axis('off')\n",
    "    plt.title('The Truth (is out there)')\n",
    "    \n",
    "    ax2 = plt.subplot(2,3,2, sharex=ax1, sharey=ax1)\n",
    "    img2 = plt.imshow(l_hat_1[0,0,:,:])\n",
    "    plt.axis('off')\n",
    "    plt.title(estimator_1.__name__ + ' l-estimate')\n",
    "    \n",
    "    ax3 = plt.subplot(2,3,3, sharex=ax1, sharey=ax1)\n",
    "    img3 = plt.imshow(l_hat_2[0,0,:,:])\n",
    "    plt.axis('off')\n",
    "    plt.title(estimator_2.__name__ + ' l-estimate')\n",
    "    \n",
    "    ax4 = plt.subplot(2,3,4, sharex=ax1, sharey=ax1)\n",
    "    img4 = plt.imshow(smibi_data[0,0,:,:])\n",
    "    plt.axis('off')\n",
    "    plt.title('\"MIBI\" data')\n",
    "    \n",
    "    ax5 = plt.subplot(2,3,5)\n",
    "    img5 = plt.imshow(h1, origin='low', extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], cmap='inferno', **kwargs)\n",
    "    plt.plot([0,xedges[-1]], [0,xedges[-1]], color='green')\n",
    "    plt.plot([0,xedges[-1]], [0,-xedges[-1]], color='green')\n",
    "    ax5.set_aspect(xedges[-1]/yedges[-1])\n",
    "    plt.xlim(xedges[0], xedges[-1])\n",
    "    plt.ylim(yedges[0], yedges[-1])\n",
    "    plt.title(estimator_1.__name__ + ' error dist.')\n",
    "    \n",
    "    ax6 = plt.subplot(2,3,6, sharex=ax5, sharey=ax5)\n",
    "    img5 = plt.imshow(h2, origin='low', extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], cmap='inferno', **kwargs)\n",
    "    plt.plot([0,xedges[-1]], [0,xedges[-1]], color='green')\n",
    "    plt.plot([0,xedges[-1]], [0,-xedges[-1]], color='green')\n",
    "    ax6.set_aspect(xedges[-1]/yedges[-1])\n",
    "    plt.xlim(xedges[0], xedges[-1])\n",
    "    plt.ylim(yedges[0], yedges[-1])\n",
    "    plt.title(estimator_2.__name__ + ' error dist.')\n",
    "    # fig.colorbar(img2, ax=ax3)\n",
    "    \n",
    "    return [img1, img2, img3, img4]\n",
    "\n",
    "def update_clims(imgs, cmax):\n",
    "    for img in imgs:\n",
    "        img.set_clim(0, cmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imgs = plot_2_error_hist(gdt_estimator, {'bs': bs, 'ts': ts}, test_ds.images[6].unsqueeze(0), net_estimator, {'network': estimator}, gdt_estimator, {'bs': bs, 'ts': ts}, vmax=.1)\n",
    "# imgs = plot_2_error_hist(net_estimator, {'network': estimator}, test_ds.images[6].unsqueeze(0), net_estimator, {'network': estimator}, naive_estimator, {'blur': 1}, vmax=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_clims(imgs, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_fig = plt.figure(figsize=(7,5))\n",
    "# plt.plot(stupid_estimator_results['error_hist'], color=[1,0,1], label='stupid_estimator')\n",
    "plt.plot(naive_estimator_1_results['error_hist'], color=[1,0,0], label='naive_estimator_1')\n",
    "# plt.plot(naive_estimator_5_results['error_hist'], color=[1,.5,0], label='naive_estimator_5')\n",
    "# plt.plot(naive_estimator_20_results['error_hist'], color=[1,1,0], label='naive_estimator_20')\n",
    "plt.plot(gdt_estimator_results['error_hist'], color=[0,0,1], label='gdt_estimator')\n",
    "plt.plot(net_estimator_results['error_hist'], color=[0,1,0], label='net_estimator')\n",
    "plt.legend()\n",
    "plt.xticks(range(0,20,4))\n",
    "plt.xlim(0,19)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data\n",
    "global max_abs_val\n",
    "global idx\n",
    "\n",
    "idx = 4\n",
    "data = gdt_estimator_results\n",
    "max_abs_val = 0.0\n",
    "\n",
    "for i in range(20):\n",
    "    sub_maxval = torch.max(torch.abs(data['l_est_hist'][i][idx,:,:,:]-data['l_est_hist'][0][idx,:,:,:])).item()\n",
    "    max_abs_val = np.max([max_abs_val, sub_maxval])\n",
    "    \n",
    "print(max_abs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "\n",
    "diff_fig = plt.figure(figsize=(15,5))\n",
    "# max_abs_val = 2\n",
    "\n",
    "\n",
    "def update(index=0):\n",
    "    global data\n",
    "    global max_abs_val\n",
    "    global idx\n",
    "    \n",
    "    ax1 = plt.subplot(1,3,1)\n",
    "    plt.imshow(data['l_est_hist'][0][idx,0,:,:])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    diff = data['l_est_hist'][index]-data['l_est_hist'][0]\n",
    "    ax2 = plt.subplot(1,3,2, sharex=ax1, sharey=ax1)\n",
    "    plt.imshow(diff[idx,0,:,:], vmin=-max_abs_val, vmax=max_abs_val)\n",
    "    plt.axis('off')\n",
    "    plt.title('Error on Iteration ' + str(index))\n",
    "    \n",
    "    ax3 = plt.subplot(1,3,3, sharex=ax1, sharey=ax1)\n",
    "    plt.imshow(data['l_est_hist'][index][idx,0,:,:])\n",
    "    plt.axis('off')\n",
    "    \n",
    "interact(update, index=widgets.IntSlider(value=0, min=0, max=19, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_fig = plt.figure(figsize=(12,6))\n",
    "gdt_est = gdt_estimator_results['l_est_hist'][0][idx,0,:,:]\n",
    "net_est = net_estimator_results['l_est_hist'][0][idx,0,:,:]\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "plt.imshow(gdt_est)\n",
    "ax2 = plt.subplot(1,2,2, sharex=ax1, sharey=ax1)\n",
    "plt.imshow(net_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/home/hazmat/GitHub/Denoisotron/data/shirley_point/TIFs/CD45.tif'\n",
    "data = io.imread(data_path).astype(int)\n",
    "data = torch.tensor(data, dtype=torch.uint8).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = [10];\n",
    "while bs[-1]>.7:\n",
    "    bs.append(bs[-1]*0.8)\n",
    "\n",
    "ts = torch.tensor(bs).float()**-2\n",
    "bs = np.array(bs)\n",
    "ts = ts.numpy()\n",
    "print(bs)\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_est, c_img = utils.estimate_lambda(data, bs, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(lambda_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
