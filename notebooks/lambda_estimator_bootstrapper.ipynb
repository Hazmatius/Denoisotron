{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import torch\n",
    "import kornia\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from helpers import Trainer\n",
    "from helpers import Logger\n",
    "from helpers import Trial\n",
    "from mibi_dataloader import MIBIData\n",
    "from modules import SelfSupervisedEstimator\n",
    "from criteria import SelfSupervisedEstimatorLoss\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idx(string):\n",
    "    parts = re.split('-', string)\n",
    "    return int(parts[1])\n",
    "\n",
    "def gdt_estimator(samples):\n",
    "    bs = [10];\n",
    "    while bs[-1]>.7:\n",
    "        bs.append(bs[-1]*0.8)\n",
    "    ts = torch.tensor(bs).float()**-2\n",
    "    bs = np.array(bs)\n",
    "    ts = ts.numpy()\n",
    "    l_ests = torch.zeros(samples.shape)\n",
    "    for i in range(samples.size(0)):\n",
    "        l_ests[i,:,:,:], cimg = utils.estimate_lambda(samples[i,0,:,:], bs, ts)\n",
    "        print('\\rProcessing.......' + str(100 * i / samples.size(0)) + '%', end='')\n",
    "    return l_ests\n",
    "\n",
    "def net_estimator(samples, network):\n",
    "    network.cuda()\n",
    "    l_ests = torch.zeros(samples.shape)\n",
    "    for i in range(samples.size(0)):\n",
    "        with torch.no_grad():\n",
    "            limg = network.process(samples[i,:,:,:].unsqueeze(0).cuda())\n",
    "            limg[limg<0] = 0\n",
    "            l_ests[i,:,:,:] = limg\n",
    "            print('\\rProcessing.......' + str(100 * i / samples.size(0)) + '%', end='')\n",
    "    return l_ests\n",
    "\n",
    "def gdt_synthesize_dataset(source_data_dir, main_dir, number):\n",
    "    print('    Loading empirical MIBI data...')\n",
    "    empirical_ds = MIBIData(folder=source_data_dir, crop=31, scale=1, stride=8, number=number)\n",
    "    print('    Creating synthetic lambda data using gdt-estimator...')\n",
    "    lambda_1 = gdt_estimator(empirical_ds.images)\n",
    "    lambda_1_ds = MIBIData(images=lambda_1, labels='None', source=empirical_ds.source, crop=31, scale=1, stride=8)\n",
    "    print('    Saving synthetic lambda data to disk...')\n",
    "    lambda_1_ds.pickle(main_dir + 'synthetic_datasets/synthetic_lambda-1')\n",
    "    print('    Initial synthetic lambda data saved.')\n",
    "    return lambda_1_ds\n",
    "\n",
    "def net_synthesize_dataset(source_data_dir, main_dir, network, number, index):\n",
    "    print('    Loading empirical MIBI data...')\n",
    "    empirical_ds = MIBIData(folder=source_data_dir, crop=31, scale=1, stride=8, number=number)\n",
    "    print('    Creating synthetic lambda data using net-estimator...')\n",
    "    lambda_i = net_estimator(empirical_ds.images, network)\n",
    "    lambda_i_ds = MIBIData(images=lambda_i, labels='None', source=empirical_ds.source, crop=31, scale=1, stride=8)\n",
    "    print('    Saving synthetic lambda data to disk...')\n",
    "    lambda_i_ds.pickle(main_dir + 'synthetic_datasets/synthetic_lambda-' + str(index))\n",
    "    return lambda_i_ds\n",
    "    \n",
    "def find_latest(thing, directory):\n",
    "    if thing is 'model':\n",
    "        elements = os.listdir(directory + 'models')\n",
    "    elif thing is 'dataset':\n",
    "        elements = os.listdir(directory + 'synthetic_datasets')\n",
    "    else:\n",
    "        print('Invalid argument in \"find_latest\", must search for a \"model\" or a \"dataset\".')\n",
    "    if len(elements)==0:\n",
    "        return 0, ''\n",
    "    else:\n",
    "        element_idxs = [get_idx(i) for i in elements]\n",
    "        last_element_index = max(element_idxs)\n",
    "        element_index = element_idxs.index(last_element_index)\n",
    "        element_name = elements[element_index]\n",
    "        return last_element_index, element_name\n",
    "\n",
    "# train the network on the dataset using the training parameters\n",
    "def train_estimator(estimator, dataset, estimator_train_args):\n",
    "    torch.cuda.empty_cache()\n",
    "    estimator.cuda()\n",
    "    estimator_logger = Logger(['loss'])\n",
    "    estimator_trainer = Trainer()\n",
    "    estimator_criterion = SelfSupervisedEstimatorLoss()\n",
    "    estimator_train_args['continue'] = False\n",
    "    dataset.set_crop(estimator_train_args['crop'])\n",
    "    \n",
    "    print('Training lambda-estimator...')\n",
    "    estimator_trainer.train(estimator, dataset, estimator_criterion, estimator_logger, main_dir + 'estimator/models/', **estimator_train_args)\n",
    "    return estimator\n",
    "    \n",
    "\n",
    "# returns a model and a dataset, does some housekeeping\n",
    "def load_model_and_dataset(main_dir, source_data_dir, number):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model_idx, model_name = find_latest('model', main_dir)\n",
    "    if model_idx==0:\n",
    "        print('    No pretrained model found, initializing random network.')\n",
    "        estimator = SelfSupervisedEstimator()\n",
    "    else:\n",
    "        print('    Pretrained model found, loading network.')\n",
    "        estimator = SelfSupervisedEstimator.load_model(main_dir + 'models/', model_name)\n",
    "        \n",
    "    ds_idx, ds_name = find_latest('dataset', main_dir)\n",
    "    \n",
    "    if ds_idx==0:\n",
    "        print('    No synthetic data, generating synthetic data...')\n",
    "        if model_idx==0:\n",
    "            print('    No pretrained model found, using GDT estimator...')\n",
    "            dataset = gdt_synthesize_dataset(source_data_dir, main_dir, number)\n",
    "        else:\n",
    "            print('    Using pretrained model to synthesize data...')\n",
    "            dataset = net_synthesize_dataset(source_data_dir, main_dir, estimator, number, model_idx+1)\n",
    "    else:\n",
    "        if model_idx<ds_idx:\n",
    "            dataset = MIBIData.depickle(main_dir + '/synthetic_datasets/' + ds_name)\n",
    "        else: # model_idx > ds_idx\n",
    "            dataset = net_synthesize_dataset(source_data_dir, main_dir, estimator, number, model_idx)\n",
    "            \n",
    "    return estimator, model_idx, dataset\n",
    "\n",
    "    \n",
    "def bootstrap_estimator(main_dir, source_data_dir, number, estimator_train_args, N):\n",
    "    print('Bootstrapping lambda-estimator function...')\n",
    "    for i in range(N):\n",
    "        estimator, model_idx, dataset = load_model_and_dataset(main_dir, source_data_dir, number)\n",
    "        estimator = train_estimator(estimator, dataset, estimator_train_args)\n",
    "        estimator.save_model(main_dir + 'models/', 'estimator-' + str(model_idx+1))\n",
    "        print('Finished iteration.')\n",
    "        print()\n",
    "    print('Finished with bootstrapping protocol.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_dir = '/home/hazmat/GitHub/Denoisotron/estimator/'\n",
    "source_data_dir = '/home/hazmat/GitHub/Denoisotron/data/traindat/'\n",
    "number = 10000\n",
    "\n",
    "N = 10\n",
    "\n",
    "estimator_train_args = dict()\n",
    "estimator_train_args['lr'] = 0.0001\n",
    "estimator_train_args['batch_size'] = 100\n",
    "estimator_train_args['epochs'] = 1\n",
    "estimator_train_args['report'] = 5\n",
    "estimator_train_args['crop'] = 121\n",
    "estimator_train_args['clip'] = 1\n",
    "estimator_train_args['decay'] = 0\n",
    "estimator_train_args['restart'] = False\n",
    "estimator_train_args['epoch_frac'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping lambda-estimator function...\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.014462148648064546                                                                                                                  \n",
      "trained in 2790.41073012352 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.013910954567653932                                                                                                                  \n",
      "trained in 2824.1667172908783 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.013835047865835424                                                                                                                  \n",
      "trained in 2830.702178001404 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.013430923908526309                                                                                                                  \n",
      "trained in 2794.0665848255157 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.013708305798112901                                                                                                                  \n",
      "trained in 2797.4109251499176 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.014309414077054659                                                                                                                  \n",
      "trained in 2796.493827342987 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.013744894796204655                                                                                                                  \n",
      "trained in 2795.236522912979 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.014402887357474137                                                                                                                  \n",
      "trained in 2795.520896434784 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.014425918399074754                                                                                                                  \n",
      "trained in 2804.729453086853 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.014875869572393367                                                                                                                  \n",
      "trained in 2764.322425842285 seconds\n",
      "Finished iteration.\n",
      "\n",
      "Finished with bootstrapping protocol.\n"
     ]
    }
   ],
   "source": [
    "bootstrap_estimator(main_dir, source_data_dir, number, estimator_train_args, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
