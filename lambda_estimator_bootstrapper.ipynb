{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import torch\n",
    "import kornia\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from helpers import Trainer\n",
    "from helpers import Logger\n",
    "from helpers import Trial\n",
    "from mibi_dataloader import MIBIData\n",
    "from modules import SelfSupervisedEstimator\n",
    "from criteria import SelfSupervisedEstimatorLoss\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(string):\n",
    "    parts = re.split('-', string)\n",
    "    return int(parts[1])\n",
    "\n",
    "def gdt_estimator(samples):\n",
    "    bs = [10];\n",
    "    while bs[-1]>.7:\n",
    "        bs.append(bs[-1]*0.8)\n",
    "    ts = torch.tensor(bs).float()**-2\n",
    "    bs = np.array(bs)\n",
    "    ts = ts.numpy()\n",
    "    l_ests = torch.zeros(samples.shape)\n",
    "    for i in range(samples.size(0)):\n",
    "        l_ests[i,:,:,:], cimg = utils.estimate_lambda(samples[i,0,:,:], bs, ts)\n",
    "        print('\\rProcessing.......' + str(100 * i / samples.size(0)) + '%', end='')\n",
    "    return l_ests\n",
    "\n",
    "def net_estimator(samples, network):\n",
    "    network.cuda()\n",
    "    l_ests = torch.zeros(samples.shape)\n",
    "    for i in range(samples.size(0)):\n",
    "        with torch.no_grad():\n",
    "            limg = network.process(samples[i,:,:,:].unsqueeze(0).cuda())\n",
    "            limg[limg<0] = 0\n",
    "            l_ests[i,:,:,:] = limg\n",
    "            print('\\rProcessing.......' + str(100 * i / samples.size(0)) + '%', end='')\n",
    "    return l_ests\n",
    "\n",
    "def gdt_synthesize_dataset(source_data_dir, main_dir, number):\n",
    "    print('    Loading empirical MIBI data...')\n",
    "    empirical_ds = MIBIData(folder=source_data_dir, crop=31, scale=1, stride=8, number=number)\n",
    "    print('    Creating synthetic lambda data using gdt-estimator...')\n",
    "    lambda_1 = gdt_estimator(empirical_ds.images)\n",
    "    lambda_1_ds = MIBIData(images=lambda_1, labels='None', source=empirical_ds.source, crop=31, scale=1, stride=8)\n",
    "    print('    Saving synthetic lambda data to disk...')\n",
    "    lambda_1_ds.pickle(main_dir + 'synthetic_datasets/synthetic_lambda-1')\n",
    "    print('    Initial synthetic lambda data saved.')\n",
    "    return lambda_1_ds\n",
    "\n",
    "def net_synthesize_dataset(source_data_dir, main_dir, network, number, index):\n",
    "    print('    Loading empirical MIBI data...')\n",
    "    empirical_ds = MIBIData(folder=source_data_dir, crop=31, scale=1, stride=8, number=number)\n",
    "    print('    Creating synthetic lambda data using net-estimator...')\n",
    "    lambda_i = net_estimator(empirical_ds.images, network)\n",
    "    lambda_i_ds = MIBIData(images=lambda_i, labels='None', source=empirical_ds.source, crop=31, scale=1, stride=8)\n",
    "    print('    Saving synthetic lambda data to disk...')\n",
    "    lambda_i_ds.pickle(main_dir + 'synthetic_datasets/synthetic_lambda-' + str(index))\n",
    "    return lambda_i_ds\n",
    "    \n",
    "def find_latest(thing, directory):\n",
    "    if thing is 'model':\n",
    "        elements = os.listdir(directory + 'models')\n",
    "    elif thing is 'dataset':\n",
    "        elements = os.listdir(directory + 'synthetic_datasets')\n",
    "    else:\n",
    "        print('Invalid argument in \"find_latest\", must search for a \"model\" or a \"dataset\".')\n",
    "    if len(elements)==0:\n",
    "        return 0, ''\n",
    "    else:\n",
    "        element_idxs = [get_idx(i) for i in elements]\n",
    "        last_element_index = max(element_idxs)\n",
    "        element_index = element_idxs.index(last_element_index)\n",
    "        element_name = elements[element_index]\n",
    "        return last_element_index, element_name\n",
    "\n",
    "# train the network on the dataset using the training parameters\n",
    "def train_estimator(estimator, dataset, estimator_train_args):\n",
    "    torch.cuda.empty_cache()\n",
    "    estimator.cuda()\n",
    "    estimator_logger = Logger(['loss'])\n",
    "    estimator_trainer = Trainer()\n",
    "    estimator_criterion = SelfSupervisedEstimatorLoss()\n",
    "    estimator_train_args['continue'] = False\n",
    "    dataset.set_crop(estimator_train_args['crop'])\n",
    "    \n",
    "    print('Training lambda-estimator...')\n",
    "    estimator_trainer.train(estimator, dataset, estimator_criterion, estimator_logger, main_dir + 'estimator/models/', **estimator_train_args)\n",
    "    return estimator\n",
    "    \n",
    "\n",
    "# returns a model and a dataset, does some housekeeping\n",
    "def load_model_and_dataset(main_dir, source_data_dir, number):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model_idx, model_name = find_latest('model', main_dir)\n",
    "    if model_idx==0:\n",
    "        print('    No pretrained model found, initializing random network.')\n",
    "        estimator = SelfSupervisedEstimator()\n",
    "    else:\n",
    "        print('    Pretrained model found, loading network.')\n",
    "        estimator = SelfSupervisedEstimator.load_model(main_dir + 'models/', model_name)\n",
    "        \n",
    "    ds_idx, ds_name = find_latest('dataset', main_dir)\n",
    "    \n",
    "    if ds_idx==0:\n",
    "        print('    No synthetic data, generating synthetic data...')\n",
    "        if model_idx==0:\n",
    "            print('    No pretrained model found, using GDT estimator...')\n",
    "            dataset = gdt_synthesize_dataset(source_data_dir, main_dir, number)\n",
    "        else:\n",
    "            print('    Using pretrained model to synthesize data...')\n",
    "            dataset = net_synthesize_dataset(source_data_dir, main_dir, estimator, number, model_idx+1)\n",
    "    else:\n",
    "        if model_idx<ds_idx:\n",
    "            dataset = MIBIData.depickle(main_dir + '/synthetic_datasets/' + ds_name)\n",
    "        else: # model_idx > ds_idx\n",
    "            dataset = net_synthesize_dataset(source_data_dir, main_dir, estimator, number, model_idx)\n",
    "            \n",
    "    return estimator, model_idx, dataset\n",
    "\n",
    "    \n",
    "def bootstrap_estimator(main_dir, source_data_dir, number, estimator_train_args, N):\n",
    "    print('Bootstrapping lambda-estimator function...')\n",
    "    for i in range(N):\n",
    "        estimator, model_idx, dataset = load_model_and_dataset(main_dir, source_data_dir, number)\n",
    "        estimator = train_estimator(estimator, dataset, estimator_train_args)\n",
    "        estimator.save_model(main_dir + 'models/', 'estimator-' + str(model_idx+1))\n",
    "        print('Finished iteration.')\n",
    "        print()\n",
    "    print('Finished with bootstrapping protocol.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_dir = '/home/hazmat/GitHub/Denoisotron/estimator/'\n",
    "source_data_dir = '/home/hazmat/GitHub/Denoisotron/data/traindat/'\n",
    "number = 10000\n",
    "\n",
    "N = 3\n",
    "\n",
    "estimator_train_args = dict()\n",
    "estimator_train_args['lr'] = 0.0001\n",
    "estimator_train_args['batch_size'] = 100\n",
    "estimator_train_args['epochs'] = 3\n",
    "estimator_train_args['report'] = 5\n",
    "estimator_train_args['crop'] = 121\n",
    "estimator_train_args['clip'] = 1\n",
    "estimator_train_args['decay'] = 0\n",
    "estimator_train_args['restart'] = False\n",
    "estimator_train_args['epoch_frac'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping lambda-estimator function...\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "Epoch:0 > < 0.01796143888560686                                                                                                                    \n",
      "Epoch:1 > < 0.017905510717023638                                                                                                                   \n",
      "Epoch:2 > < 0.01788106080373723                                                                                                                    \n",
      "trained in 83510.86734604836 seconds\n",
      "Finished iteration.\n",
      "\n",
      "    Pretrained model found, loading network.\n",
      "    Loading empirical MIBI data...\n",
      "Loading.......99.9899989999%998%%%%900\n",
      "There are  35996400 samples\n",
      "    Creating synthetic lambda data using net-estimator...\n",
      "Processing.......99.9899989999%900%%%%\n",
      "There are  35996400 samples\n",
      "    Saving synthetic lambda data to disk...\n",
      "2304\n",
      "Training lambda-estimator...\n",
      "2304\n",
      "    Minibatch:4004/230376 > < loss: 0.01445,                                                                                                     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8a78625bfc2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbootstrap_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_train_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-65822a23d1bb>\u001b[0m in \u001b[0;36mbootstrap_estimator\u001b[0;34m(main_dir, source_data_dir, number, estimator_train_args, N)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_train_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'models/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished iteration.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-65822a23d1bb>\u001b[0m in \u001b[0;36mtrain_estimator\u001b[0;34m(estimator, dataset, estimator_train_args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training lambda-estimator...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mestimator_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'estimator/models/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mestimator_train_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/Denoisotron/helpers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_set, criterion, logger, home_dir, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                     \u001b[0mmodel_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0merror_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_vars\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;31m# we assume that loss is always a key in the dict returned by the criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/Denoisotron/criteria.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecon_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2188\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2189\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2190\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bootstrap_estimator(main_dir, source_data_dir, number, estimator_train_args, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
